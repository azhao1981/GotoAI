# 数学基础

## 概述

本章参考[<<转型AI｜非科班却拿下阿里云栖一等奖，他经历的坑足够你学100天>>](https://www.toutiao.com/i6488554603049648654/)

对三个基础数学概念进行详细描述

 1 懂得矩阵运算的基本计算方法，能够手动计算[3×4]×[4×3]的矩阵，并明白为什么会得到一个[3×3]的矩阵。

 2 能够计算基本的先验及后验概率。

 3 懂得导数的基本含义，明白为什么可以利用导数来计算梯度，并实现迭代优化。

## 矩阵

[笔记](./线性代数.md)

```python
""" 矩阵例子 """
import numpy as np

matrix_a = np.mat('4 3 2; 2 1 4; 1 2 3')
matrix_b = np.mat('1 2 3; 2 3 4; 3 4 1')

print(matrix_a)
[[4 3 2]
 [2 1 4]
 [1 2 3]]
print(matrix_b)
[[1 2 3]
 [2 3 4]
 [3 4 1]]
print(matrix_a * matrix_b)
[[16 25 26]
 [16 23 14]
 [14 20 14]]

print(matrix_a - matrix_b)

print(matrix_a + matrix_b)

```

## 先验似然后验

先验概率： 事件发生前的预判概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如P(x),P(y)。

后验概率： 事件发生后求的反向条件概率；或者说，基于先验概率求得的反向条件概率。概率形式与条件概率相同。

条件概率： 一个事件发生后另一个事件发生的概率。一般的形式为P(x|y)表示y发生的条件下x发生的概率。

贝叶斯公式：

    P(y|x) = ( P(x|y) * P(y) ) / P(x)

这里：

    P(y|x) 是后验概率，一般是我们求解的目标。

    P(x|y) 是条件概率，又叫似然概率，一般是通过历史数据统计得到。一般不把它叫做先验概率，但从定义上也符合先验定义。

    P(y) 是先验概率，一般都是人主观给出的。贝叶斯中的先验概率一般特指它。

    P(x) 其实也是先验概率，只是在贝叶斯的很多应用中不重要（因为只要最大后验不求绝对值），需要时往往用全概率公式计算得到。

实例：假设y是文章种类，是一个枚举值；x是向量，表示文章中各个单词的出现次数。

在拥有训练集的情况下，显然除了后验概率P(y|x)中的x来自一篇新文章无法得到，p(x),p(y),p(x|y)都是可以在抽样集合上统计出的。

最大似然理论：

    认为P(x|y)最大的类别y，就是当前文档所属类别。即Max P(x|y) = Max p(x1|y)*p(x2|y)*...p(xn|y), for all y

贝叶斯理论：

    认为需要增加先验概率p(y)，因为有可能某个y是很稀有的类别几千年才看见一次，即使P(x|y)很高，也很可能不是它。

    所以y = Max P(x|y) * P(y), 其中p(y)一般是数据集里统计出来的。

从上例来讲，贝叶斯理论显然更合理一些；但实际中很多先验概率是拍脑袋得出的（不准），有些甚至是为了方便求解方便生造出来的（硬凑），那有先验又有什么好处呢？一般攻击贝叶斯都在于这一点。

[贝叶斯定理](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86)

[贝叶斯概率](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A6%82%E7%8E%87)

![先验似然后验](./images/先验.jpg)

![先验似然后验2](./images/先验2.jpg)

[贝叶斯推断如何更新后验概率？](https://www.zhihu.com/question/27398304)

## 导数及梯度下降算法

[导数](https://zh.wikipedia.org/wiki/%E5%AF%BC%E6%95%B0#.E5.AF.BC.E6.95.B0.E7.9A.84.E8.AE.B0.E6.B3.95)

这个给出的是程序员的视角:(nice)

[Gradient Descent 梯度下降法](https://ctmakro.github.io/site/on_learning/gd.html)

梯度下降法可以帮助我们找到某个函数的极小值或者最小值。

[机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)](http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html)

[梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)
